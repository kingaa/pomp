---
title: "Iterated Filtering 2 Pseudocode and Example"
author: "Aaron A. King and Edward Ionides"
params:
  prefix: mif2
output:
  html_document:
    theme: default
    toc: yes
bibliography: pomp.bib
csl: jss.csl
---

[Licensed under the Creative Commons Attribution-NonCommercial license](http://creativecommons.org/licenses/by-nc/4.0/).
Please share and remix noncommercially, mentioning its origin.  
![CC-BY_NC](https://kingaa.github.io/images/cc-by-nc.png)

This document was produced using **pomp** version `r packageVersion("pomp")`.

```{r knitr-opts,include=FALSE,purl=FALSE,cache=FALSE}
source("setup.R", local = knitr::knit_global())
```
```{r prelims,echo=FALSE,cache=FALSE}
library(ggplot2)
library(knitr)
theme_set(theme_bw())
library(pomp)
stopifnot(packageVersion("pomp")>="4.2")
options(
  keep.source=TRUE,
  stringsAsFactors=FALSE,
  encoding="UTF-8",
  scipen=5,
  pomp_archive_dir="results/mif2"
)
set.seed(1332379783L)
```

Iterated filtering is a technique for maximizing the likelihood obtained by filtering.
In **pomp**, it is the particle filter that is iterated.
The iterated filtering of @Ionides2006 is implemented in the `mif` function.
@Ionides2015 describe an improvement on the original [@Ionides2006] algorithm.
This "IF2" algorithm is implemented in the `mif2` function.

## IF2 algorithm pseudocode

__model input__:
Simulators for $f_{X_0}(x_0;\theta)$ and $f_{X_n|X_{n-1}}(x_n| x_{n-1}; \theta)$;
evaluator for $f_{Y_n|X_n}(y_n| x_n;\theta)$;
data, $y^*_{1:N}$ 

__algorithmic parameters__:
Number of iterations, $M$;
number of particles, $J$;
initial parameter swarm, $\{\Theta^0_j, j=1,\dots,J\}$;
perturbation density, $h_n(\theta|\varphi;\sigma)$;
perturbation scale, $\sigma_{1{:}M}$ 

__output__:
Final parameter swarm, $\{\Theta^M_j, j=1,\dots,J\}$ 


1. $\quad$ For $m$ in $1{:} M$
2. $\quad\quad\quad$ $\Theta^{F,m}_{0,j}\sim h_0(\theta|\Theta^{m-1}_{j}; \sigma_m)$ for $j$ in $1{:} J$
3. $\quad\quad\quad$ $X_{0,j}^{F,m}\sim f_{X_0}(x_0 ; \Theta^{F,m}_{0,j})$ for $j$ in $1{:} J$
4. $\quad\quad\quad$ For $n$ in $1{:} N$
5. $\quad\quad\quad\quad\quad$ $\Theta^{P,m}_{n,j}\sim h_n(\theta|\Theta^{F,m}_{n-1,j},\sigma_m)$ for $j$ in $1{:} J$
6. $\quad\quad\quad\quad\quad$ $X_{n,j}^{P,m}\sim f_{X_n|X_{n-1}}(x_n | X^{F,m}_{n-1,j}; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$
7. $\quad\quad\quad\quad\quad$ $w_{n,j}^m = f_{Y_n|X_n}(y^*_n| X_{n,j}^{P,m} ; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$
8. $\quad\quad\quad\quad\quad$ Draw $k_{1{:}J}$ with $P[k_j=i]=  w_{n,i}^m\Big/\sum_{u=1}^J w_{n,u}^m$
9.  $\quad\quad\quad\quad\quad$ $\Theta^{F,m}_{n,j}=\Theta^{P,m}_{n,k_j}$ and $X^{F,m}_{n,j}=X^{P,m}_{n,k_j}$ for $j$ in $1{:} J$
10. $\quad\quad\quad$ End For
11. $\quad\quad\quad$ Set $\Theta^{m}_{j}=\Theta^{F,m}_{N,j}$ for $j$ in $1{:} J$
12. $\quad$ End For

## An example

The following constructs the Gompertz example that is provided with **pomp** (see `?gompertz`) and extracts the parameters at which the data were generated.
It is meant to be directly comparable with the example displayed in the @King2016 [*J. Stat. Softw.* paper](https://kingaa.github.io/pomp/vignettes/pompjss.pdf).

```{r gompertz-init,results="hide"}
library(pomp)
gompertz() -> gomp
theta <- coef(gomp)
theta.true <- theta
```
```{r gompertz-sim,include=FALSE}
gomp |>
  window(start=1) |>
  simulate(seed=340398091L) -> gomp
```

Let's use IF2 to obtain an approximate MLE for these data.
We'll initialize the algorithm at several starting points around `theta.true` and just estimate the parameters $r$, $\tau$, and $\sigma$:

```{r gompertz-mif2-1,results='hide'}
library(foreach)
library(doParallel)
registerDoParallel()

estpars <- c("r","sigma","tau")
```
```{r gompertz-mif2-2,results='hide',eval=FALSE,purl=FALSE}
library(doRNG)
registerDoRNG(525386942)

foreach(i=1:10,.inorder=FALSE) %dopar% {
  theta.guess <- theta.true
  theta.guess[estpars] <- rlnorm(
    n=length(estpars),
    meanlog=log(theta.guess[estpars]),
    sdlog=1
  )
  gomp |>
    mif2(
      Nmif=50,
      params=theta.guess,
      rw.sd=rw.sd(r=0.02,sigma=0.02,tau=0.05),
      cooling.fraction.50=0.95,
      Np=2000
    ) |>
    continue(Nmif=50,cooling.fraction=0.8) |>
    continue(Nmif=50,cooling.fraction=0.6) |>
    continue(Nmif=50,cooling.fraction=0.2) -> m1
  ll <- replicate(n=10,logLik(pfilter(m1,Np=10000)))
  list(mif=m1,ll=logmeanexp(ll,se=TRUE))
} -> mf
```
```{r gompertz-mif2-2-eval,eval=TRUE,purl=TRUE,include=FALSE}
bake(file="gompertz-mif2.rds",{
  <<gompertz-mif2-2>>
}) -> mf
```
```{r gompertz-mif-3}
lls <- sapply(mf,getElement,"ll")
best <- which.max(sapply(mf,getElement,"ll")[1,])
theta.mif <- coef(mf[[best]]$mif)

replicate(10,logLik(pfilter(gomp,params=theta.mif,Np=10000))) |>
  logmeanexp(se=TRUE) -> pf.loglik.mif
```

Note that `gompertz()` uses parameter transformations to ensure that the search for the MLE is constrained to positive parameters.
See the **pomp** documentation (`?pomp`) and the section on Parameter Transformations in the [Getting Started vignette](https://kingaa.github.io/pomp/vignettes/getting_started.html)).

Each of the `r length(mf)` `mif2` runs ends up at a different point estimate.
We focus on that with the highest estimated likelihood, having evaluated the likelihood several times to reduce the Monte Carlo error in the likelihood evaluation.
The particle filter produces an unbiased estimate of the likelihood; 
therefore, we will average the likelihoods, not the log likelihoods.

```{r kf,include=FALSE}
kalman.filter <- function (Y, X0, r, K, sigma, tau) {
  ntimes <- length(Y)
  sigma.sq <- sigma^2
  tau.sq <- tau^2
  cond.loglik <- numeric(ntimes)
  filter.mean <- numeric(ntimes)
  pred.mean <- numeric(ntimes)
  pred.var <- numeric(ntimes)
  m <- log(X0)
  v <- 0
  S <- exp(-r)
  for (k in seq_len(ntimes)) {
    pred.mean[k] <- M <- (1-S)*log(K) + S*m
    pred.var[k] <- V <- S*v*S+sigma.sq
    q <- V+tau.sq
    r <- log(Y[k])-M
    cond.loglik[k] <- dnorm(x=log(Y[k]),mean=M,sd=sqrt(q),log=TRUE)-log(Y[k])
    q <- 1/V+1/tau.sq
    filter.mean[k] <- m <- (log(Y[k])/tau.sq+M/V)/q
    v <- 1/q
  }
  list(
    pred.mean=pred.mean,
    pred.var=pred.var,
    filter.mean=filter.mean,
    cond.loglik=cond.loglik,
    loglik=sum(cond.loglik)
  )
}

##' 'kalman' evaluates gompertz likelihood parameters X0 and K are fixed at 1.
##' Other parameters are taken from x (or, if not in x, from params).
kalman <- function (x, object, params) {
  Y <- obs(object)
  p <- params
  p[names(x)] <- x
  X0 <- 1
  r <- p["r"]
  K <- 1
  sigma <- p["sigma"]
  tau <- p["tau"]
  -kalman.filter(Y, X0, r, K, sigma, tau)$loglik
}

##' Exact log likelihood at the true parameters
loglik.truth <- -kalman(coef(gomp),gomp,coef(gomp))
loglik.mif <- -kalman(theta.mif,gomp,coef(gomp))

kalm.fit1 <- optim(
  par=theta.true[estpars],
  fn=kalman,
  object=gomp,
  params=coef(gomp),
  hessian=TRUE,
  control=list(trace=0)
)

theta.mle <- coef(gomp)
theta.mle[estpars] <- kalm.fit1$par
loglik.mle <- -kalm.fit1$value
```

```{r gomp-post-mif2,include=FALSE}
replicate(n=10,logLik(pfilter(gomp,Np=10000))) |>
  logmeanexp(se=TRUE) -> pf.loglik.truth
replicate(n=10,logLik(pfilter(gomp,params=theta.mle,Np=10000))) |>
  logmeanexp(se=TRUE) -> pf.loglik.mle

rbind(`Truth`=theta.true[estpars],
  `Exact MLE`=theta.mle[estpars],
  `IF2 MLE`=theta.mif[estpars]) |> round(digits=4) |>
  cbind(`$\\hat{\\ell}$`=round(c(pf.loglik.truth[1],pf.loglik.mle[1],pf.loglik.mif[1]),2),
    `s.e. $\\hat{\\ell}$`=round(c(pf.loglik.truth[2],pf.loglik.mle[2],pf.loglik.mif[2]),2),
    `$\\ell$`=round(c(loglik.truth,loglik.mle,loglik.mif),2)
  ) -> results.table
```

Convergence plots can be used to help diagnose convergence of the iterated filtering algorithm.
Something like the following can be obtained by executing `plot(mf)`.

```{r mif2-plot,echo=FALSE,cache=FALSE,fig.height=6}
op <- par(mfrow=c(4,1),mar=c(3,3,0,0),mgp=c(2,1,0),bty='l')
loglik <- sapply(mf,function(x)traces(x$mif,"loglik"))
r <- sapply(mf,function(x)traces(x$mif,"r"))
sigma <- sapply(mf,function(x)traces(x$mif,"sigma"))
tau <- sapply(mf,function(x)traces(x$mif,"tau"))
matplot(loglik,type='l',lty=1,xlab="",ylab=expression(log~L),xaxt='n',ylim=max(loglik,na.rm=T)+c(-12,3))
matplot(r,type='l',lty=1,xlab="",ylab=expression(r),xaxt='n')
matplot(sigma,type='l',lty=1,xlab="",ylab=expression(sigma),xaxt='n')
matplot(tau,type='l',lty=1,xlab="MIF iteration",ylab=expression(tau))
par(op)
```

Here is a summary of the results:
```{r first-mif-results-table,echo=FALSE,cache=FALSE}
kable(results.table)
```

Compare this with the [performance of IF1 (`mif`) on the same task](https://kingaa.github.io/pomp/vignettes/pompjss.pdf#page=20).

The fact that the likelihood, $\ell$, at the IF2 MLE is higher than it is at the truth is evidence that this parameter is actually close to the true MLE.

## Links

[**pomp** homepage](https://kingaa.github.io/pomp/)  
[**R** codes for this document](https://raw.githubusercontent.com/kingaa/pomp/gh-pages/vignettes/mif2.R)  

## References
